{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas\n",
    "import xgboost\n",
    "import numpy\n",
    "import textblob, string\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = open('corpus',encoding='utf-8').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(content[1:])\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 0, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "valid_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',tokenizer=lambda doc: doc, lowercase=False)\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7500x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 361188 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000,tokenizer=lambda doc: doc, lowercase=False)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "xtrain_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-4c4b896d32fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# ngram level tf-idf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtfidf_vect_ngram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'\\w{1,}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtfidf_vect_ngram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mxtrain_tfidf_ngram\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtfidf_vect_ngram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mxvalid_tfidf_ngram\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtfidf_vect_ngram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m-> 1361\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000,lowercase=False)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000,tokenizer=lambda doc: doc, lowercase=False)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec', encoding='utf-8')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['char_count'] = trainDF['text'].apply(lambda x: len(\"\".join(x)))\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Stuning, even, for, the, non-gamer:, This, so...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>347</td>\n",
       "      <td>80</td>\n",
       "      <td>4.283951</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, best, soundtrack, ever, to, anything.:, ...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>413</td>\n",
       "      <td>97</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Amazing!:, This, soundtrack, is, my, favorite...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>632</td>\n",
       "      <td>129</td>\n",
       "      <td>4.861538</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Excellent, Soundtrack:, I, truly, like, this,...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>626</td>\n",
       "      <td>118</td>\n",
       "      <td>5.260504</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Remember,, Pull, Your, Jaw, Off, The, Floor, ...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>395</td>\n",
       "      <td>87</td>\n",
       "      <td>4.488636</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label  char_count  \\\n",
       "0  [Stuning, even, for, the, non-gamer:, This, so...  __label__2         347   \n",
       "1  [The, best, soundtrack, ever, to, anything.:, ...  __label__2         413   \n",
       "2  [Amazing!:, This, soundtrack, is, my, favorite...  __label__2         632   \n",
       "3  [Excellent, Soundtrack:, I, truly, like, this,...  __label__2         626   \n",
       "4  [Remember,, Pull, Your, Jaw, Off, The, Floor, ...  __label__2         395   \n",
       "\n",
       "   word_count  word_density  punctuation_count  title_word_count  \\\n",
       "0          80      4.283951                  0                10   \n",
       "1          97      4.214286                  0                 7   \n",
       "2         129      4.861538                  1                24   \n",
       "3         118      5.260504                  2                52   \n",
       "4          87      4.488636                  0                30   \n",
       "\n",
       "   upper_case_word_count  \n",
       "0                      3  \n",
       "1                      3  \n",
       "2                      4  \n",
       "3                      4  \n",
       "4                      0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "c = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>pron_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Stuning, even, for, the, non-gamer:, This, so...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>347</td>\n",
       "      <td>80</td>\n",
       "      <td>4.283951</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, best, soundtrack, ever, to, anything.:, ...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>413</td>\n",
       "      <td>97</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Amazing!:, This, soundtrack, is, my, favorite...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>632</td>\n",
       "      <td>129</td>\n",
       "      <td>4.861538</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Excellent, Soundtrack:, I, truly, like, this,...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>626</td>\n",
       "      <td>118</td>\n",
       "      <td>5.260504</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Remember,, Pull, Your, Jaw, Off, The, Floor, ...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>395</td>\n",
       "      <td>87</td>\n",
       "      <td>4.488636</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[an, absolute, masterpiece:, I, am, quite, sur...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>684</td>\n",
       "      <td>142</td>\n",
       "      <td>4.783217</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Buyer, beware:, This, is, a, self-published, ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>600</td>\n",
       "      <td>139</td>\n",
       "      <td>4.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Glorious, story:, I, loved, Whisper, of, the,...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>418</td>\n",
       "      <td>105</td>\n",
       "      <td>3.943396</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[A, FIVE, STAR, BOOK:, I, just, finished, read...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>422</td>\n",
       "      <td>103</td>\n",
       "      <td>4.057692</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Whispers, of, the, Wicked, Saints:, This, was...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>239</td>\n",
       "      <td>63</td>\n",
       "      <td>3.734375</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[The, Worst!:, A, complete, waste, of, time., ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>182</td>\n",
       "      <td>35</td>\n",
       "      <td>5.055556</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Great, book:, This, was, a, great, book,I, ju...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>375</td>\n",
       "      <td>96</td>\n",
       "      <td>3.865979</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Great, Read:, I, thought, this, book, was, br...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>238</td>\n",
       "      <td>59</td>\n",
       "      <td>3.966667</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Oh, please:, I, guess, you, have, to, be, a, ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>400</td>\n",
       "      <td>93</td>\n",
       "      <td>4.255319</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[Awful, beyond, belief!:, I, feel, I, have, to...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>561</td>\n",
       "      <td>132</td>\n",
       "      <td>4.218045</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[Don't, try, to, fool, us, with, fake, reviews...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>252</td>\n",
       "      <td>53</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[A, romantic, zen, baseball, comedy:, When, yo...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>570</td>\n",
       "      <td>118</td>\n",
       "      <td>4.789916</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Fashionable, Compression, Stockings!:, After,...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>544</td>\n",
       "      <td>115</td>\n",
       "      <td>4.689655</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[Jobst, UltraSheer, Thigh, High:, Excellent, p...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>300</td>\n",
       "      <td>72</td>\n",
       "      <td>4.109589</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[sizes, recomended, in, the, size, chart, are,...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>193</td>\n",
       "      <td>49</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[mens, ultrasheer:, This, model, may, be, ok, ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>342</td>\n",
       "      <td>72</td>\n",
       "      <td>4.684932</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[Delicious, cookie, mix:, I, thought, it, was,...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>680</td>\n",
       "      <td>164</td>\n",
       "      <td>4.121212</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[Another, Abysmal, Digital, Copy:, Rather, tha...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>832</td>\n",
       "      <td>165</td>\n",
       "      <td>5.012048</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[A, fascinating, insight, into, the, life, of,...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>655</td>\n",
       "      <td>137</td>\n",
       "      <td>4.746377</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[i, liked, this, album, more, then, i, thought...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>216</td>\n",
       "      <td>53</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[Problem, with, charging, smaller, AAAs:, I, h...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>515</td>\n",
       "      <td>118</td>\n",
       "      <td>4.327731</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[Works,, but, not, as, advertised:, I, bought,...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>440</td>\n",
       "      <td>101</td>\n",
       "      <td>4.313725</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[Disappointed:, I, read, the, reviews,made, my...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>236</td>\n",
       "      <td>51</td>\n",
       "      <td>4.538462</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[Oh, dear:, I, was, excited, to, find, a, book...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>518</td>\n",
       "      <td>102</td>\n",
       "      <td>5.029126</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[Based, on, the, reviews, here, I, bought, one...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>701</td>\n",
       "      <td>161</td>\n",
       "      <td>4.327160</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[Incorrect, disc!:, I, am, a, big, JVC, fan,, ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>373</td>\n",
       "      <td>92</td>\n",
       "      <td>4.010753</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[happy, with, it...but:, I'm, a, JVC, nut...I,...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>562</td>\n",
       "      <td>133</td>\n",
       "      <td>4.194030</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[should, be, titled, \"Hollywood, Debacle\":, Th...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>254</td>\n",
       "      <td>56</td>\n",
       "      <td>4.456140</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[Is, this, great, TV???, You, bet, it, is:, Ho...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>710</td>\n",
       "      <td>165</td>\n",
       "      <td>4.277108</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[Nothing, you, don't, already, know:, If, you,...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>152</td>\n",
       "      <td>31</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[Unfortunately, it, wasn't, entertaining, in, ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>556</td>\n",
       "      <td>134</td>\n",
       "      <td>4.118519</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[Rochelle, explains, It, All, for, You:, Wonde...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>565</td>\n",
       "      <td>116</td>\n",
       "      <td>4.829060</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[More, great, playing:, Larry's, work, for, th...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>373</td>\n",
       "      <td>78</td>\n",
       "      <td>4.721519</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[Doesn't, work, on, a, Mac:, It, clearly, says...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>135</td>\n",
       "      <td>37</td>\n",
       "      <td>3.552632</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[Very, Frustrating:, My, three, year, old, son...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>309</td>\n",
       "      <td>72</td>\n",
       "      <td>4.232877</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[Mind, numbing:, This, game, makes, you, do, t...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>235</td>\n",
       "      <td>62</td>\n",
       "      <td>3.730159</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[Cannot, recommend:, As, a, former, Alaskan,, ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>808</td>\n",
       "      <td>162</td>\n",
       "      <td>4.957055</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[Alaska, sourdough:, REad, most, of, the, book...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>148</td>\n",
       "      <td>33</td>\n",
       "      <td>4.352941</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[Old, and, good:, This, book, is, worth, to, k...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>215</td>\n",
       "      <td>53</td>\n",
       "      <td>3.981481</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[autumn:, got, this, for, my, daughter, in, NC...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>89</td>\n",
       "      <td>22</td>\n",
       "      <td>3.869565</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[Fast, mp3, download, of, music, waxed, decade...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>323</td>\n",
       "      <td>72</td>\n",
       "      <td>4.424658</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[Maybe, I, got, a, bad, one.:, My, experience:...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>461</td>\n",
       "      <td>114</td>\n",
       "      <td>4.008696</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[Sylvania, 6620LDG, 20\", Flat, Panel, LCD, TV,...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>444</td>\n",
       "      <td>95</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[TERRIBLE!!, DO, NOT, BUY, THIS:, I, bought, t...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>192</td>\n",
       "      <td>46</td>\n",
       "      <td>4.085106</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[***, BEWARE, ***:, This, TV, is, set, so, tha...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>397</td>\n",
       "      <td>101</td>\n",
       "      <td>3.892157</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text       label  char_count  \\\n",
       "0   [Stuning, even, for, the, non-gamer:, This, so...  __label__2         347   \n",
       "1   [The, best, soundtrack, ever, to, anything.:, ...  __label__2         413   \n",
       "2   [Amazing!:, This, soundtrack, is, my, favorite...  __label__2         632   \n",
       "3   [Excellent, Soundtrack:, I, truly, like, this,...  __label__2         626   \n",
       "4   [Remember,, Pull, Your, Jaw, Off, The, Floor, ...  __label__2         395   \n",
       "5   [an, absolute, masterpiece:, I, am, quite, sur...  __label__2         684   \n",
       "6   [Buyer, beware:, This, is, a, self-published, ...  __label__1         600   \n",
       "7   [Glorious, story:, I, loved, Whisper, of, the,...  __label__2         418   \n",
       "8   [A, FIVE, STAR, BOOK:, I, just, finished, read...  __label__2         422   \n",
       "9   [Whispers, of, the, Wicked, Saints:, This, was...  __label__2         239   \n",
       "10  [The, Worst!:, A, complete, waste, of, time., ...  __label__1         182   \n",
       "11  [Great, book:, This, was, a, great, book,I, ju...  __label__2         375   \n",
       "12  [Great, Read:, I, thought, this, book, was, br...  __label__2         238   \n",
       "13  [Oh, please:, I, guess, you, have, to, be, a, ...  __label__1         400   \n",
       "14  [Awful, beyond, belief!:, I, feel, I, have, to...  __label__1         561   \n",
       "15  [Don't, try, to, fool, us, with, fake, reviews...  __label__1         252   \n",
       "16  [A, romantic, zen, baseball, comedy:, When, yo...  __label__2         570   \n",
       "17  [Fashionable, Compression, Stockings!:, After,...  __label__2         544   \n",
       "18  [Jobst, UltraSheer, Thigh, High:, Excellent, p...  __label__2         300   \n",
       "19  [sizes, recomended, in, the, size, chart, are,...  __label__1         193   \n",
       "20  [mens, ultrasheer:, This, model, may, be, ok, ...  __label__1         342   \n",
       "21  [Delicious, cookie, mix:, I, thought, it, was,...  __label__2         680   \n",
       "22  [Another, Abysmal, Digital, Copy:, Rather, tha...  __label__1         832   \n",
       "23  [A, fascinating, insight, into, the, life, of,...  __label__2         655   \n",
       "24  [i, liked, this, album, more, then, i, thought...  __label__2         216   \n",
       "25  [Problem, with, charging, smaller, AAAs:, I, h...  __label__1         515   \n",
       "26  [Works,, but, not, as, advertised:, I, bought,...  __label__1         440   \n",
       "27  [Disappointed:, I, read, the, reviews,made, my...  __label__1         236   \n",
       "28  [Oh, dear:, I, was, excited, to, find, a, book...  __label__1         518   \n",
       "29  [Based, on, the, reviews, here, I, bought, one...  __label__2         701   \n",
       "30  [Incorrect, disc!:, I, am, a, big, JVC, fan,, ...  __label__1         373   \n",
       "31  [happy, with, it...but:, I'm, a, JVC, nut...I,...  __label__2         562   \n",
       "32  [should, be, titled, \"Hollywood, Debacle\":, Th...  __label__1         254   \n",
       "33  [Is, this, great, TV???, You, bet, it, is:, Ho...  __label__2         710   \n",
       "34  [Nothing, you, don't, already, know:, If, you,...  __label__1         152   \n",
       "35  [Unfortunately, it, wasn't, entertaining, in, ...  __label__1         556   \n",
       "36  [Rochelle, explains, It, All, for, You:, Wonde...  __label__2         565   \n",
       "37  [More, great, playing:, Larry's, work, for, th...  __label__2         373   \n",
       "38  [Doesn't, work, on, a, Mac:, It, clearly, says...  __label__1         135   \n",
       "39  [Very, Frustrating:, My, three, year, old, son...  __label__1         309   \n",
       "40  [Mind, numbing:, This, game, makes, you, do, t...  __label__1         235   \n",
       "41  [Cannot, recommend:, As, a, former, Alaskan,, ...  __label__1         808   \n",
       "42  [Alaska, sourdough:, REad, most, of, the, book...  __label__2         148   \n",
       "43  [Old, and, good:, This, book, is, worth, to, k...  __label__2         215   \n",
       "44  [autumn:, got, this, for, my, daughter, in, NC...  __label__2          89   \n",
       "45  [Fast, mp3, download, of, music, waxed, decade...  __label__2         323   \n",
       "46  [Maybe, I, got, a, bad, one.:, My, experience:...  __label__1         461   \n",
       "47  [Sylvania, 6620LDG, 20\", Flat, Panel, LCD, TV,...  __label__2         444   \n",
       "48  [TERRIBLE!!, DO, NOT, BUY, THIS:, I, bought, t...  __label__1         192   \n",
       "49  [***, BEWARE, ***:, This, TV, is, set, so, tha...  __label__1         397   \n",
       "\n",
       "    word_count  word_density  punctuation_count  title_word_count  \\\n",
       "0           80      4.283951                  0                10   \n",
       "1           97      4.214286                  0                 7   \n",
       "2          129      4.861538                  1                24   \n",
       "3          118      5.260504                  2                52   \n",
       "4           87      4.488636                  0                30   \n",
       "5          142      4.783217                  0                14   \n",
       "6          139      4.285714                  0                16   \n",
       "7          105      3.943396                  0                13   \n",
       "8          103      4.057692                  1                15   \n",
       "9           63      3.734375                  0                 8   \n",
       "10          35      5.055556                  0                 5   \n",
       "11          96      3.865979                  0                 6   \n",
       "12          59      3.966667                  0                10   \n",
       "13          93      4.255319                  1                11   \n",
       "14         132      4.218045                  0                12   \n",
       "15          53      4.666667                  0                 4   \n",
       "16         118      4.789916                  0                 8   \n",
       "17         115      4.689655                  0                18   \n",
       "18          72      4.109589                  0                 8   \n",
       "19          49      3.860000                  0                 2   \n",
       "20          72      4.684932                  1                 9   \n",
       "21         164      4.121212                  0                13   \n",
       "22         165      5.012048                  0                10   \n",
       "23         137      4.746377                  0                19   \n",
       "24          53      4.000000                  0                 1   \n",
       "25         118      4.327731                  0                 9   \n",
       "26         101      4.313725                  0                 9   \n",
       "27          51      4.538462                  0                 4   \n",
       "28         102      5.029126                  0                13   \n",
       "29         161      4.327160                  0                11   \n",
       "30          92      4.010753                  0                11   \n",
       "31         133      4.194030                  0                 5   \n",
       "32          56      4.456140                  2                 8   \n",
       "33         165      4.277108                  0                26   \n",
       "34          31      4.750000                  0                 2   \n",
       "35         134      4.118519                  0                25   \n",
       "36         116      4.829060                  1                13   \n",
       "37          78      4.721519                  0                11   \n",
       "38          37      3.552632                  1                 8   \n",
       "39          72      4.232877                  1                 6   \n",
       "40          62      3.730159                  0                 6   \n",
       "41         162      4.957055                  0                19   \n",
       "42          33      4.352941                  0                 5   \n",
       "43          53      3.981481                  0                 4   \n",
       "44          22      3.869565                  0                 1   \n",
       "45          72      4.424658                  0                 7   \n",
       "46         114      4.008696                  1                14   \n",
       "47          95      4.625000                  0                13   \n",
       "48          46      4.085106                  0                 3   \n",
       "49         101      3.892157                  0                 8   \n",
       "\n",
       "    upper_case_word_count  noun_count  verb_count  adj_count  adv_count  \\\n",
       "0                       3           0           0          0          0   \n",
       "1                       3           0           0          0          0   \n",
       "2                       4           0           0          0          0   \n",
       "3                       4           0           0          0          0   \n",
       "4                       0           0           0          0          0   \n",
       "5                       3           0           0          0          0   \n",
       "6                       4           0           0          0          0   \n",
       "7                       6           0           0          0          0   \n",
       "8                      13           0           0          0          0   \n",
       "9                       2           0           0          0          0   \n",
       "10                      2           0           0          0          0   \n",
       "11                      0           0           0          0          0   \n",
       "12                      5           0           0          0          0   \n",
       "13                      5           0           0          0          0   \n",
       "14                      7           0           0          0          0   \n",
       "15                      0           0           0          0          0   \n",
       "16                      8           0           0          0          0   \n",
       "17                     11           0           0          0          0   \n",
       "18                      2           0           0          0          0   \n",
       "19                      2           0           0          0          0   \n",
       "20                      2           0           0          0          0   \n",
       "21                      9           0           0          0          0   \n",
       "22                      2           0           0          0          0   \n",
       "23                      4           0           0          0          0   \n",
       "24                      1           0           0          0          0   \n",
       "25                      5           0           0          0          0   \n",
       "26                      6           0           0          0          0   \n",
       "27                      3           0           0          0          0   \n",
       "28                      2           0           0          0          0   \n",
       "29                     11           0           0          0          0   \n",
       "30                     11           0           0          0          0   \n",
       "31                     12           0           0          0          0   \n",
       "32                      1           0           0          0          0   \n",
       "33                     12           0           0          0          0   \n",
       "34                      0           0           0          0          0   \n",
       "35                     17           0           0          0          0   \n",
       "36                      5           0           0          0          0   \n",
       "37                      1           0           0          0          0   \n",
       "38                      1           0           0          0          0   \n",
       "39                      1           0           0          0          0   \n",
       "40                      0           0           0          0          0   \n",
       "41                      3           0           0          0          0   \n",
       "42                      1           0           0          0          0   \n",
       "43                      0           0           0          0          0   \n",
       "44                      1           0           0          0          0   \n",
       "45                      1           0           0          0          0   \n",
       "46                     11           0           0          0          0   \n",
       "47                      6           0           0          0          0   \n",
       "48                      9           0           0          0          0   \n",
       "49                      7           0           0          0          0   \n",
       "\n",
       "    pron_count  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            0  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  \n",
       "10           0  \n",
       "11           0  \n",
       "12           0  \n",
       "13           0  \n",
       "14           0  \n",
       "15           0  \n",
       "16           0  \n",
       "17           0  \n",
       "18           0  \n",
       "19           0  \n",
       "20           0  \n",
       "21           0  \n",
       "22           0  \n",
       "23           0  \n",
       "24           0  \n",
       "25           0  \n",
       "26           0  \n",
       "27           0  \n",
       "28           0  \n",
       "29           0  \n",
       "30           0  \n",
       "31           0  \n",
       "32           0  \n",
       "33           0  \n",
       "34           0  \n",
       "35           0  \n",
       "36           0  \n",
       "37           0  \n",
       "38           0  \n",
       "39           0  \n",
       "40           0  \n",
       "41           0  \n",
       "42           0  \n",
       "43           0  \n",
       "44           0  \n",
       "45           0  \n",
       "46           0  \n",
       "47           0  \n",
       "48           0  \n",
       "49           0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"function to check and get the part of speech tag count of a words in a given sentence\")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x='''function to check and get the part of speech tag count of a words in a given sentence'''\n",
    "wiki = textblob.TextBlob(x)\n",
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('function', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('check', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('get', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('part', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('speech', 'NN'),\n",
       " ('tag', 'NN'),\n",
       " ('count', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('words', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('given', 'VBN'),\n",
       " ('sentence', 'NN')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.translate import Translator\n",
    "t = Translator()\n",
    "t.translate('hello', from_lang='en', to_lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.detect(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic Modelling - #Train a LDA model\n",
    "\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20,learning_method='online', max_iter=20)\n",
    "\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"memory drivers window inspiring recommended. seeking fate God's Sam Buddy\",\n",
       " 'the and a to I of is this it in',\n",
       " 'door socks breaking BD Would rule again! presenting individuals happens.',\n",
       " 'Sony money, Lola enough, taste whom Being Tomb original, Run',\n",
       " 'DVD: cake mark pulling edges Truth 11 Christmas, Jewish Korean',\n",
       " 'dialogue herself tree network dialog actors. video, training print. thriller',\n",
       " 'upon much, Clan Despite sections Auel manual shocked said. directed',\n",
       " 'installed blade airbed Super leak offered Emma cedar Dear leaks',\n",
       " 'battery computer charger power laptop adapter cord charge Windows wireless',\n",
       " 'rice fun. Pretty Catholic character. Classic reason. cooker techno Kate',\n",
       " 'u Nice instead. World dishes better: collar Jackson selections combat',\n",
       " 'film young film. of performance John Manson his political cast',\n",
       " 'Totally cincher right? Small charm. ive praising Metal sea Firewire',\n",
       " 'beat solid \"real\" downloading Whitney rock. involving marry Asimov\\'s raider',\n",
       " 'game. novels onto David blue Stargate Robert stock hooked twists',\n",
       " 'hair checked office results station Brando CD. did, sentence sold',\n",
       " 'THE A I THIS AND OF IS NOT TO IT',\n",
       " 'de la condition. en economics La Christmas Has con que',\n",
       " 'provides 1984 compared tool Thomas product: was. Winston Orwell Guy',\n",
       " 'outstanding viewer companion Chandler \"Making senior ......... Daughter storyline. Brian']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sugar is bad to consume. My sister likes to have sugar, but not my father.',\n",
       " 'My father spends a lot of time driving my sister around to dance practice.',\n",
       " 'Doctors suggest that driving may cause increased stress and blood pressure.',\n",
       " 'Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.',\n",
       " 'Health experts say that Sugar is not good for your lifestyle.']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LDA explanation in python\n",
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "doc_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sugar', 'bad', 'consume', 'sister', 'like', 'sugar', 'father'],\n",
       " ['father',\n",
       "  'spends',\n",
       "  'lot',\n",
       "  'time',\n",
       "  'driving',\n",
       "  'sister',\n",
       "  'around',\n",
       "  'dance',\n",
       "  'practice'],\n",
       " ['doctor',\n",
       "  'suggest',\n",
       "  'driving',\n",
       "  'may',\n",
       "  'cause',\n",
       "  'increased',\n",
       "  'stress',\n",
       "  'blood',\n",
       "  'pressure'],\n",
       " ['sometimes',\n",
       "  'feel',\n",
       "  'pressure',\n",
       "  'perform',\n",
       "  'well',\n",
       "  'school',\n",
       "  'father',\n",
       "  'never',\n",
       "  'seems',\n",
       "  'drive',\n",
       "  'sister',\n",
       "  'better'],\n",
       " ['health', 'expert', 'say', 'sugar', 'good', 'lifestyle']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and Preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2)],\n",
       " [(2, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(8, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1)],\n",
       " [(2, 1),\n",
       "  (4, 1),\n",
       "  (18, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1)],\n",
       " [(5, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing Document-Term Matrix\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA Model\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "ldamodel = Lda(doc_term_matrix,num_topics=3, id2word = dictionary, passes=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.064*\"sugar\" + 0.064*\"sister\" + 0.064*\"father\"'),\n",
       " (1, '0.045*\"pressure\" + 0.045*\"father\" + 0.045*\"sister\"'),\n",
       " (2, '0.029*\"sugar\" + 0.029*\"father\" + 0.029*\"sister\"')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result\n",
    "ldamodel.print_topics(num_topics=3, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    \n",
    "    classifier.fit(feature_vector_train,label)\n",
    "    \n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=1)\n",
    "        \n",
    "    return metrics.accuracy_score(predictions,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.8304\n",
      "NB, WordLevel TF-IDF:  0.8356\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes \n",
    "\n",
    "#naive bayes on Count Vectors\n",
    "\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(),xtrain_count,train_y,xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, count vectors 0.8496\n",
      "LR, WordLevel TF-IDF:  0.8396\n"
     ]
    }
   ],
   "source": [
    "#Linear Classifier - Logistic Regression\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, count vectors\",accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.5004\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.724\n",
      "RF, WordLevel TF-IDF:  0.724\n"
     ]
    }
   ],
   "source": [
    "#Bagging Model \n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xxkem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.7632\n",
      "Xgb, WordLevel TF-IDF:  0.7684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xxkem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print( \"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7500x78684 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 459720 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count.tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 124s 17ms/step - loss: 0.4177\n",
      "NN, Ngram Level TF IDF Vectors 0.51\n"
     ]
    }
   ],
   "source": [
    "#shallow Neural Networks\n",
    "\n",
    "def create_model_architecture(input_size):\n",
    "    #create input layer\n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    #create hidden layer\n",
    "    hidden_layer = layers.Dense(5000, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    #create output layer\n",
    "    output_layer = layers.Dense(1,activation=\"sigmoid\")(hidden_layer)\n",
    "    \n",
    "    classifier = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    classifier.compile(optimizer=optimizers.Adam(),loss='binary_crossentropy')\n",
    "\n",
    "    return classifier\n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf.shape[1])\n",
    "accuracy = train_model(classifier,xtrain_tfidf, train_y, xvalid_tfidf, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 17s 2ms/step - loss: 0.6090\n",
      "CNN, Word Embeddings 0.51\n"
     ]
    }
   ],
   "source": [
    "#deep neural network\n",
    "\n",
    "def create_cnn():\n",
    "    #Add input layer\n",
    "    input_layer = layers.Input((70,))\n",
    "    \n",
    "    #Add the word embedding layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    #Add the convolutional layer\n",
    "    conv_layer = layers.Convolution1D(100,3, activation=\"relu\")(embedding_layer)\n",
    "    \n",
    "    #Add the pooling layer\n",
    "    pooling_layer = layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    \n",
    "    #Add the output layer\n",
    "    output_layer1 = layers.Dense(50,activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer_2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    \n",
    "    #compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer_2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier=create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 37s 5ms/step - loss: 0.6120\n",
      "RNN-LSTM, Word Embeddings 0.51\n"
     ]
    }
   ],
   "source": [
    "#Recurrent Neural Network – LSTM\n",
    "def create_rnn_lstm():\n",
    "    #add an input layer\n",
    "    input_layer = layers.Input((70,))\n",
    "    \n",
    "    #add the word embedding layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    #Add LStM layer\n",
    "    lstm_layer  = layers.LSTM(100)(embedding_layer)\n",
    "    \n",
    "    #Add the output layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    \n",
    "    #compile the Model\n",
    "    model = models.Model(inputs=input_layer, outputs = output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(),loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 36s 5ms/step - loss: 0.6197\n",
      "RNN-GRU, Word Embeddings 0.51\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_gru():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"RNN-GRU, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 62s 8ms/step - loss: 0.6309\n",
      "RNN-Bidirectional, Word Embeddings 0.51\n"
     ]
    }
   ],
   "source": [
    "def create_bidirectional_rnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 23s 3ms/step - loss: 0.6096\n",
      "CNN, Word Embeddings 0.51\n"
     ]
    }
   ],
   "source": [
    "def create_rcnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
